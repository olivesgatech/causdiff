---------------------------------------
Begin Slurm Prolog: Apr-20-2025 00:54:22
Job ID:    2500036
User ID:   skim3513
Account:   ece
Job name:  run-causal.sh
Partition: coe-gpu
QOS:       coe-ice
---------------------------------------
*-----------------------------*
    model  =  bit-diff-pred-tcn
    layer_type  =  gated
    bz  =  8
    num_stages  =  5
    obs_stages  =  0
    ant_stages  =  5
    kernel_size  =  3
    num_layers  =  9
    lr  =  0.0005
    ds  =  bf
    sample_rate  =  3
    use_features  =  True
    channel_dropout_prob  =  0.4
    split  =  4
*-----------------------------*
Dir : 202504192145_bf_bit-diff-pred-tcn_os_0_as_5_m_dim_64_lt_gated_pvo_inp_ch_dr_ns_1000_lt_l2_obj_pred_x0_cond_x0_True_split_4
Number of classes : 48
Number of classes : 10
Dataset: bf
Mode : train
Vid list file : ./datasets/breakfast/splits/train.split4.bundle
Observation perc : 0
Dataset: bf
Mode : eval
Vid list file : ./datasets/breakfast/splits/test.split4.bundle
Observation perc : 0.2
Dataset: bf
Mode : eval
Vid list file : ./datasets/breakfast/splits/test.split4.bundle
Observation perc : 0.3
Num classes : 48
Loss type : l2
Objective: pred_x0
Beta schedule : cosine
Start Training...
training in the loop for  426  number of loops
[epoch 0.000000]: epoch loss = 1.925477, ce loss = 0.000000, past acc = 0.001238, fut acc = 0.000000 
[epoch 0.011737]: epoch loss = 1.109154, ce loss = 0.000000, past acc = 0.049203, fut acc = 0.019718 
[epoch 0.023474]: epoch loss = 0.877782, ce loss = 0.000000, past acc = 0.038692, fut acc = 0.072694 
[epoch 0.035211]: epoch loss = 0.753215, ce loss = 0.000000, past acc = 0.050223, fut acc = 0.060393 
[epoch 0.046948]: epoch loss = 0.698450, ce loss = 0.000000, past acc = 0.054030, fut acc = 0.067781 
[epoch 0.058685]: epoch loss = 0.649713, ce loss = 0.000000, past acc = 0.065513, fut acc = 0.073295 
[epoch 0.070423]: epoch loss = 0.607588, ce loss = 0.000000, past acc = 0.064617, fut acc = 0.082627 
[epoch 0.082160]: epoch loss = 0.584919, ce loss = 0.000000, past acc = 0.080740, fut acc = 0.094404 
[epoch 0.093897]: epoch loss = 0.561779, ce loss = 0.000000, past acc = 0.093526, fut acc = 0.106587 
[epoch 0.105634]: epoch loss = 0.540242, ce loss = 0.000000, past acc = 0.092477, fut acc = 0.108540 
[epoch 0.117371]: epoch loss = 0.530217, ce loss = 0.000000, past acc = 0.099901, fut acc = 0.117700 
[epoch 0.129108]: epoch loss = 0.515001, ce loss = 0.000000, past acc = 0.107972, fut acc = 0.124819 
[epoch 0.140845]: epoch loss = 0.502684, ce loss = 0.000000, past acc = 0.110036, fut acc = 0.129881 
[epoch 0.152582]: epoch loss = 0.489042, ce loss = 0.000000, past acc = 0.115643, fut acc = 0.141381 
[epoch 0.164319]: epoch loss = 0.480014, ce loss = 0.000000, past acc = 0.125912, fut acc = 0.147204 
[epoch 0.176056]: epoch loss = 0.475277, ce loss = 0.000000, past acc = 0.131141, fut acc = 0.146944 
[epoch 0.187793]: epoch loss = 0.464782, ce loss = 0.000000, past acc = 0.137861, fut acc = 0.156981 
[epoch 0.199531]: epoch loss = 0.457103, ce loss = 0.000000, past acc = 0.146714, fut acc = 0.155586 
[epoch 0.211268]: epoch loss = 0.447976, ce loss = 0.000000, past acc = 0.151864, fut acc = 0.158695 
[epoch 0.223005]: epoch loss = 0.443649, ce loss = 0.000000, past acc = 0.156414, fut acc = 0.153853 
[epoch 0.234742]: epoch loss = 0.438801, ce loss = 0.000000, past acc = 0.159822, fut acc = 0.152881 
[epoch 0.246479]: epoch loss = 0.437585, ce loss = 0.000000, past acc = 0.165771, fut acc = 0.153914 
Traceback (most recent call last):
  File "./src/main-causal.py", line 221, in <module>
    trainer.train(args=args,
  File "/storage/ice1/7/2/skim3513/GTDA/src/trainers_causal.py", line 118, in train
    batch_correct_future, batch_total_future = self.train_single_batch(sample_batched, optimizer, device)
  File "/storage/ice1/7/2/skim3513/GTDA/src/trainers_causal.py", line 251, in train_single_batch
    loss, tcn_predictions = self.diffusion({'mask_past': rearrange(mask_past_tensor, 'b c t -> b t c'),
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/ice1/7/2/skim3513/GTDA/src/bit_diffusion_causal.py", line 299, in forward
    return self.p_losses(
  File "/storage/ice1/7/2/skim3513/GTDA/src/bit_diffusion_causal.py", line 252, in p_losses
    model_out, high_level_goal_softmax = self.model(
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/ice1/7/2/skim3513/GTDA/src/models_bit_diff_causal.py", line 77, in forward
    frame_wise_pred, _ = self.ms_tcn(x, t, stage_masks, high_level_goal)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/ice1/7/2/skim3513/GTDA/src/models_bit_diff_causal.py", line 137, in forward
    out, out_features = s(torch.cat((F.softmax(out, dim=1) * stage_masks[sn], x), dim=1), t, stage_masks[sn], high_level_goal)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/ice1/7/2/skim3513/GTDA/src/models_bit_diff_causal.py", line 203, in forward
    out = layer(out, time, mask, high_level_goal)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/ice1/7/2/skim3513/GTDA/src/models_bit_diff_causal.py", line 311, in forward
    out = self.goal_attn(out, high_level_goal)  # (B, C, T)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/ice1/7/2/skim3513/GTDA/src/models_bit_diff_causal.py", line 401, in forward
    z_hat, _ = self.is_attn(x_seq, x_seq, x_seq)  # (B, T, C)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1275, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/functional.py", line 5526, in multi_head_attention_forward
    attn_output_weights = softmax(attn_output_weights, dim=-1)
  File "/home/hice1/skim3513/AIFirst_F24_data/anaconda3/envs/myenv3/lib/python3.8/site-packages/torch/nn/functional.py", line 1888, in softmax
    ret = input.softmax(dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 79.22 GiB of which 289.56 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: atl1-1-03-013-3-0: task 0: Exited with exit code 1
---------------------------------------
Begin Slurm Epilog: Apr-20-2025 00:55:34
Job ID:        2500036
User ID:       skim3513
Account:       ece
Job name:      run-causal.sh
Resources:     cpu=32,gres/gpu:h100=1,mem=128G,node=1
Rsrc Used:     cput=00:40:00,vmem=0,walltime=00:01:15,mem=9732K,energy_used=0
Partition:     coe-gpu
QOS:           coe-ice
Nodes:         atl1-1-03-013-3-0
---------------------------------------
